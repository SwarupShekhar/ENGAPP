# Backend-AI Production Upgrade Guide

Your IDE has built a solid foundation. This guide helps you upgrade it to handle the issues identified in the architectural audit.

---

## ðŸŽ¯ Identified Issues & Solutions

### Issue 1: Multi-Instance Disk Cache Problem

**Current State:**
```python
# Won't work across K8s pods
self.disk_cache = Cache(settings.disk_cache_dir)
```

**Problem:**
- Each pod has its own disk
- Cache misses on pod B for data cached on pod A
- Inconsistent behavior

**Solution:** Use `enhanced_cache.py`

```python
# Auto-detects K8s and uses Redis-only mode
from improvements.enhanced_cache import cache_manager, CacheMode

# In K8s: Automatically uses Redis only
# Single instance: Uses both Redis + Disk
# Dev: Can force disk-only mode
```

**Migration Steps:**

1. **Add environment detection:**
```bash
# In .env (K8s deployment)
CACHE_MODE=redis_only  # Force Redis-only

# OR let it auto-detect (recommended)
# - Detects /var/run/secrets/kubernetes.io
# - Automatically switches to redis_only
```

2. **Update cache initialization:**
```python
# backend-ai/app/cache/manager.py
# Replace current CacheManager with:
from improvements.enhanced_cache import EnhancedCacheManager as CacheManager
```

3. **Verify in health check:**
```python
# app/api/routes/health.py
@router.get("/health/cache")
async def cache_health():
    stats = await cache_manager.get_stats()
    return {
        "mode": stats["mode"],  # Shows: redis_only in K8s
        "redis": stats["redis"],
        "disk": stats["disk"]  # None in redis_only mode
    }
```

4. **Test multi-instance:**
```bash
# Start two instances
uvicorn app.main:app --port 8001 &
uvicorn app.main:app --port 8002 &

# Cache should work across both (via Redis)
curl http://localhost:8001/api/transcribe  # Caches in Redis
curl http://localhost:8002/api/transcribe  # Hits same Redis cache
```

---

### Issue 2: Azure SDK Blocking Event Loop

**Current State:**
```python
# Wrapping sync calls in executors (works but not optimal)
result = await asyncio.to_thread(azure_speech_recognizer.recognize_once)
```

**Problem:**
- Thread pool overhead
- Limited concurrency
- GIL contention

**Solution:** Use `async_azure_speech.py`

**Benefits:**
- Dedicated thread pool for Azure operations
- Proper async patterns
- Better error handling
- Retry logic built-in
- Clean separation of concerns

**Migration Steps:**

1. **Replace Azure service:**
```python
# Old: app/services/azure_service.py
from azure.cognitiveservices.speech import SpeechRecognizer

class AzureService:
    def transcribe(self, audio_bytes):
        result = recognizer.recognize_once()  # Blocking!

# New: Use async wrapper
from improvements.async_azure_speech import azure_speech

class TranscriptionService:
    async def transcribe(self, audio_bytes, language="en-US"):
        result = await azure_speech.transcribe_from_bytes(
            audio_bytes,
            language
        )
        return result  # Already parsed
```

2. **Update pronunciation service:**
```python
# Old
def assess_pronunciation(self, audio, text):
    # Complex Azure SDK setup...
    result = recognizer.recognize_once()  # Blocking

# New
async def assess_pronunciation(self, audio, text):
    result = await azure_speech.assess_pronunciation(
        audio,
        text,
        language="en-US"
    )
    return result  # Fully parsed
```

3. **Add cleanup on shutdown:**
```python
# app/main.py
from improvements.async_azure_speech import shutdown_executor

@app.on_event("shutdown")
async def shutdown():
    await shutdown_executor()
    logger.info("Azure executor shutdown complete")
```

4. **Configure executor size:**
```python
# In async_azure_speech.py
# Adjust based on your load:
_executor = ThreadPoolExecutor(
    max_workers=8,  # Increase for high concurrency
    thread_name_prefix="azure_speech"
)
```

---

### Issue 3: Gemini JSON Parsing Brittleness

**Current State:**
```python
# Basic parsing with try-except
try:
    response_json = json.loads(gemini_response.text)
except json.JSONDecodeError:
    # Return empty or error
    return {"errors": []}
```

**Problem:**
- Gemini sometimes returns markdown wrapped JSON
- Occasionally includes comments
- May have trailing commas
- Schema fields can change
- No graceful degradation

**Solution:** Use `robust_json_parser.py`

**Features:**
- Extracts JSON from markdown code blocks
- Handles multiple JSON formats
- Schema validation with fallbacks
- Partial recovery from malformed JSON
- Domain-specific defaults for Englivo

**Migration Steps:**

1. **Update analysis service:**
```python
# Old: app/services/analysis.py
import json

async def analyze(text: str):
    response = await gemini.generate_content(prompt)
    
    try:
        result = json.loads(response.text)
    except json.JSONDecodeError:
        logger.error("JSON parse failed")
        return default_response

# New: Use robust parser
from improvements.robust_json_parser import parse_gemini_analysis

async def analyze(text: str):
    response = await gemini.generate_content(prompt)
    
    # Automatically handles all parsing edge cases
    result = parse_gemini_analysis(response.text)
    # Always returns valid structure, even if parsing fails
    
    return result
```

2. **Add Pydantic validation (optional but recommended):**
```python
# app/models/analysis.py
from pydantic import BaseModel

class AnalysisResponse(BaseModel):
    errors: List[ErrorDetail] = []
    feedback: str
    strengths: List[str] = []
    improvement_areas: List[str] = []
    recommended_tasks: List[Dict] = []

# In parser
from improvements.robust_json_parser import RobustJSONParser

result = RobustJSONParser.parse_with_recovery(
    gemini_response.text,
    schema=AnalysisResponse,  # Validates structure
    fallback={"feedback": "Analysis completed", ...}
)
```

3. **Test with malformed responses:**
```python
# tests/test_json_parser.py
def test_malformed_json():
    # Test markdown wrapped
    text = """
    ```json
    {
      "errors": [
        {"type": "grammar", "text": "test"},
      ]
    }
    ```
    """
    result = parse_gemini_analysis(text)
    assert "errors" in result
    
    # Test partial JSON
    text = '{"errors": [{"type": "grammar"'  # Incomplete
    result = parse_gemini_analysis(text)
    assert isinstance(result, dict)  # Still returns something
```

4. **Monitor parsing failures:**
```python
# Add metrics
from prometheus_client import Counter

json_parse_failures = Counter(
    'json_parse_failures_total',
    'Total JSON parsing failures',
    ['source']
)

# In parser
if all_strategies_failed:
    json_parse_failures.labels(source='gemini').inc()
    logger.error("JSON parsing exhausted all strategies")
```

---

## ðŸ”§ Additional Production Improvements

### 1. Add Circuit Breaker Pattern

Protect against cascading failures when external services go down:

```python
# improvements/circuit_breaker.py
from enum import Enum
import time
from typing import Callable, Any

class CircuitState(Enum):
    CLOSED = "closed"  # Normal operation
    OPEN = "open"      # Failures detected, stop calling
    HALF_OPEN = "half_open"  # Testing recovery

class CircuitBreaker:
    def __init__(
        self,
        failure_threshold: int = 5,
        recovery_timeout: int = 60,
        expected_exception: type = Exception
    ):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.expected_exception = expected_exception
        
        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED
    
    async def call(self, func: Callable, *args, **kwargs) -> Any:
        if self.state == CircuitState.OPEN:
            if time.time() - self.last_failure_time > self.recovery_timeout:
                self.state = CircuitState.HALF_OPEN
                logger.info("Circuit breaker entering half-open state")
            else:
                raise RuntimeError("Circuit breaker is OPEN")
        
        try:
            result = await func(*args, **kwargs)
            
            if self.state == CircuitState.HALF_OPEN:
                self.state = CircuitState.CLOSED
                self.failure_count = 0
                logger.info("Circuit breaker closed, service recovered")
            
            return result
        
        except self.expected_exception as e:
            self.failure_count += 1
            self.last_failure_time = time.time()
            
            if self.failure_count >= self.failure_threshold:
                self.state = CircuitState.OPEN
                logger.error(f"Circuit breaker opened after {self.failure_count} failures")
            
            raise

# Usage in services
gemini_breaker = CircuitBreaker(failure_threshold=5, recovery_timeout=60)

async def call_gemini_with_breaker(prompt: str):
    return await gemini_breaker.call(gemini.generate_content, prompt)
```

### 2. Add Request Deduplication

Prevent duplicate processing of identical requests:

```python
# improvements/deduplication.py
import hashlib
import asyncio
from typing import Dict, Any

class RequestDeduplicator:
    """
    Deduplicate identical in-flight requests.
    Multiple users requesting the same resource get the same result.
    """
    
    def __init__(self):
        self.in_flight: Dict[str, asyncio.Future] = {}
    
    def _generate_key(self, *args, **kwargs) -> str:
        """Generate cache key from request parameters."""
        data = {"args": args, "kwargs": kwargs}
        serialized = json.dumps(data, sort_keys=True)
        return hashlib.sha256(serialized.encode()).hexdigest()
    
    async def deduplicate(self, func, *args, **kwargs):
        """
        Execute function, deduplicating concurrent identical calls.
        """
        key = self._generate_key(*args, **kwargs)
        
        # Check if request is in flight
        if key in self.in_flight:
            logger.info(f"Deduplicating request: {key[:8]}...")
            return await self.in_flight[key]
        
        # Create future for this request
        future = asyncio.Future()
        self.in_flight[key] = future
        
        try:
            result = await func(*args, **kwargs)
            future.set_result(result)
            return result
        except Exception as e:
            future.set_exception(e)
            raise
        finally:
            # Clean up
            del self.in_flight[key]

# Usage
deduplicator = RequestDeduplicator()

@cached("transcription")
async def transcribe(audio_url: str):
    return await deduplicator.deduplicate(
        _transcribe_impl,
        audio_url
    )
```

### 3. Add Rate Limiting per User

Protect against abuse:

```python
# improvements/rate_limiter.py
import time
from typing import Dict
from collections import defaultdict, deque

class SlidingWindowRateLimiter:
    """Rate limiter using sliding window algorithm."""
    
    def __init__(self, requests_per_minute: int = 60):
        self.requests_per_minute = requests_per_minute
        self.window_size = 60  # seconds
        self.requests: Dict[str, deque] = defaultdict(deque)
    
    async def check_limit(self, user_id: str) -> bool:
        """
        Check if user is within rate limit.
        Returns True if allowed, False if rate limited.
        """
        now = time.time()
        user_requests = self.requests[user_id]
        
        # Remove old requests outside window
        while user_requests and user_requests[0] < now - self.window_size:
            user_requests.popleft()
        
        # Check limit
        if len(user_requests) >= self.requests_per_minute:
            return False
        
        # Add new request
        user_requests.append(now)
        return True
    
    def get_retry_after(self, user_id: str) -> int:
        """Get seconds until user can make another request."""
        if user_id not in self.requests or not self.requests[user_id]:
            return 0
        
        oldest_request = self.requests[user_id][0]
        retry_after = int(self.window_size - (time.time() - oldest_request))
        return max(0, retry_after)

# Middleware
rate_limiter = SlidingWindowRateLimiter(requests_per_minute=60)

@app.middleware("http")
async def rate_limit_middleware(request: Request, call_next):
    user_id = request.headers.get("X-User-ID")
    
    if user_id and not await rate_limiter.check_limit(user_id):
        retry_after = rate_limiter.get_retry_after(user_id)
        return JSONResponse(
            status_code=429,
            headers={"Retry-After": str(retry_after)},
            content={"error": "Rate limit exceeded"}
        )
    
    return await call_next(request)
```

---

## ðŸ“Š Monitoring Improvements

### Add Custom Metrics

```python
# app/core/metrics.py
from prometheus_client import Counter, Histogram, Gauge

# Service health
service_health = Gauge(
    'service_health_status',
    'Health status of external services',
    ['service']
)

# Cache metrics
cache_operations = Counter(
    'cache_operations_total',
    'Total cache operations',
    ['operation', 'result']  # operation: get/set, result: hit/miss
)

# AI operation metrics
ai_operation_duration = Histogram(
    'ai_operation_duration_seconds',
    'Duration of AI operations',
    ['operation', 'provider'],  # operation: transcribe/analyze, provider: azure/gemini
    buckets=[0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0]
)

ai_operation_errors = Counter(
    'ai_operation_errors_total',
    'Total AI operation errors',
    ['operation', 'error_type']
)

# Usage in services
from app.core.metrics import ai_operation_duration, cache_operations

async def transcribe(audio_url: str):
    with ai_operation_duration.labels(operation='transcribe', provider='azure').time():
        result = await azure_speech.transcribe(audio_url)
    return result

async def cache_get(key: str):
    result = await cache_manager.get(key)
    cache_operations.labels(
        operation='get',
        result='hit' if result else 'miss'
    ).inc()
    return result
```

### Add Alerting Rules

```yaml
# prometheus/alerts.yml
groups:
  - name: backend-ai
    rules:
      # High error rate
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        annotations:
          summary: "High error rate detected"
      
      # Slow responses
      - alert: SlowResponses
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 30
        for: 10m
        annotations:
          summary: "95th percentile response time > 30s"
      
      # Low cache hit rate
      - alert: LowCacheHitRate
        expr: |
          rate(cache_operations_total{result="hit"}[10m]) /
          rate(cache_operations_total[10m]) < 0.7
        for: 15m
        annotations:
          summary: "Cache hit rate below 70%"
      
      # Service down
      - alert: ExternalServiceDown
        expr: service_health_status == 0
        for: 5m
        annotations:
          summary: "External service {{ $labels.service }} is down"
```

---

## ðŸ§ª Testing Improvements

### Add Load Testing

```python
# tests/load_test.py
import asyncio
import time
from locust import HttpUser, task, between

class BackendAIUser(HttpUser):
    wait_time = between(1, 3)
    
    @task(3)
    def transcribe(self):
        self.client.post(
            "/api/transcribe",
            json={
                "audio_url": "https://example.com/test.wav",
                "user_id": f"test_user_{self.user_id}",
                "session_id": f"session_{time.time()}"
            },
            headers={"X-API-Key": "test_key"}
        )
    
    @task(2)
    def analyze(self):
        self.client.post(
            "/api/analyze",
            json={
                "text": "I go to the store yesterday.",
                "user_id": f"test_user_{self.user_id}",
                "session_id": f"session_{time.time()}"
            }
        )
    
    @task(1)
    def pronunciation(self):
        self.client.post(
            "/api/pronunciation",
            json={
                "audio_url": "https://example.com/test.wav",
                "reference_text": "The quick brown fox",
                "user_id": f"test_user_{self.user_id}"
            }
        )

# Run: locust -f tests/load_test.py --host=http://localhost:8001
```

### Add Integration Tests

```python
# tests/integration/test_full_flow.py
import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_full_analysis_flow(async_client: AsyncClient):
    """Test complete flow: transcribe â†’ analyze â†’ cache."""
    
    # 1. Transcribe audio
    transcribe_response = await async_client.post(
        "/api/transcribe",
        json={
            "audio_url": "https://test.com/audio.wav",
            "user_id": "test_user",
            "session_id": "test_session"
        }
    )
    assert transcribe_response.status_code == 200
    transcript = transcribe_response.json()["data"]["text"]
    
    # 2. Analyze transcript
    analyze_response = await async_client.post(
        "/api/analyze",
        json={
            "text": transcript,
            "user_id": "test_user",
            "session_id": "test_session"
        }
    )
    assert analyze_response.status_code == 200
    analysis = analyze_response.json()["data"]
    assert "cefr_assessment" in analysis
    
    # 3. Verify caching (second request should be faster)
    start = time.time()
    cached_response = await async_client.post(
        "/api/analyze",
        json={
            "text": transcript,
            "user_id": "test_user",
            "session_id": "test_session"
        }
    )
    duration = time.time() - start
    
    assert cached_response.status_code == 200
    assert duration < 1.0  # Should be instant from cache
```

---

## ðŸš€ Deployment Checklist

### Pre-Production

- [ ] Replace cache manager with K8s-aware version
- [ ] Update Azure service to use async wrapper
- [ ] Add robust JSON parser for Gemini
- [ ] Add circuit breaker for external services
- [ ] Add request deduplication
- [ ] Add per-user rate limiting
- [ ] Add custom Prometheus metrics
- [ ] Configure alerting rules
- [ ] Run load tests (target: 100 RPS)
- [ ] Run integration tests
- [ ] Document all environment variables
- [ ] Set up log aggregation (ELK/Datadog)

### Production Launch

- [ ] Deploy to staging first
- [ ] Monitor metrics for 24 hours
- [ ] Canary deployment (10% traffic)
- [ ] Monitor error rates
- [ ] Gradually increase to 100%
- [ ] Set up on-call rotation
- [ ] Document runbooks
- [ ] Create rollback plan

---

## ðŸ“ˆ Expected Improvements

After implementing these upgrades:

| Metric | Before | After |
|--------|--------|-------|
| Cache hit rate (K8s) | 40-50% | 80-90% |
| P95 latency | 8-12s | 3-5s |
| Concurrent requests | 10-20 | 50-100 |
| Error rate | 2-3% | < 0.5% |
| Gemini parsing failures | 5-10% | < 1% |
| Recovery from failures | Manual | Automatic |

---

Your implementation is already solid. These upgrades address the specific issues your IDE identified and prepare you for scale! ðŸš€